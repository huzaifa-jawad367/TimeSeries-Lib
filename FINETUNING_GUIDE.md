# Fine-tuning TimesNet: Complete Guide

## ‚úÖ Fixed Issue

**Error**: `unrecognized arguments: --pretrained_checkpoint`

**Solution**: Added `--pretrained_checkpoint` argument to `run.py` argument parser (line 40)

---

## üîß Architecture Compatibility

### ‚ö†Ô∏è CRITICAL REQUIREMENT

When fine-tuning, the pretrained model and fine-tuning script **MUST have identical architecture**:

| Parameter | Value | Where to Check |
|-----------|-------|----------------|
| `d_model` | 384 | Look for `dm384` in checkpoint path |
| `n_heads` | 6 | Look for `nh6` in checkpoint path |
| `d_ff` | 1536 | Look for `df1536` in checkpoint path |
| `e_layers` | 2 | Look for `el2` in checkpoint path |
| `d_layers` | 1 | Look for `dl1` in checkpoint path |

### Example Checkpoint Path Breakdown

```
/content/drive/MyDrive/Model pths/time_series/M4_Weekly/
short_term_forecast_m4_Weekly_TimesNet_m4_ftM_sl26_ll13_pl13_dm384_nh6_el2_dl1_df1536_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0/checkpoint.pth
                                                          ^^^^^^^   ^^^      ^^  ^^  ^^^^^^^
                                                          d_model  n_heads   e   d    d_ff
                                                           =384     =6      =2  =1   =1536
```

### ‚ùå Incompatible Example

```
Pretrained: dm32_nh8_df32    ‚Üê d_model=32, n_heads=8, d_ff=32
Fine-tune:  dm384_nh6_df1536 ‚Üê d_model=384, n_heads=6, d_ff=1536

Result: ‚ùå WILL FAIL - shapes don't match!
```

### ‚úÖ Compatible Example

```
Pretrained: dm384_nh6_df1536 ‚Üê d_model=384, n_heads=6, d_ff=1536
Fine-tune:  dm384_nh6_df1536 ‚Üê d_model=384, n_heads=6, d_ff=1536

Result: ‚úÖ SUCCESS - shapes match!
```

---

## üöÄ Step-by-Step Fine-tuning

### Step 1: Train M4 Model (Optional)

```bash
cd Time-Series-Library
bash scripts/short_term_forecast/TimesNet_M4_Weekly_Daily.sh
```

**This will create checkpoints with architecture**: `d_model=384, n_heads=6, d_ff=1536`

**Checkpoint locations**:
- Daily: `/content/drive/MyDrive/Model pths/time_series/M4_Daily/short_term_forecast_m4_Daily_*/checkpoint.pth`
- Weekly: `/content/drive/MyDrive/Model pths/time_series/M4_Weekly/short_term_forecast_m4_Weekly_*/checkpoint.pth`

### Step 2: Find Your Checkpoint

After M4 training completes, you'll see output like:

```
Updating learning rate to 0.0001
Epoch: 7 cost time: 42.3 seconds
...
path:  /content/drive/MyDrive/Model pths/time_series/M4_Weekly/short_term_forecast_m4_Weekly_TimesNet_m4_ftM_sl26_ll13_pl13_dm384_nh6_el2_dl1_df1536_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0
```

**Copy this path** and append `/checkpoint.pth`

### Step 3: Update Fine-tuning Script

Edit `TimesNet_total_finetune.sh` line 21:

```bash
PRETRAINED_CHECKPOINT='/content/drive/MyDrive/Model pths/time_series/M4_Weekly/short_term_forecast_m4_Weekly_TimesNet_m4_ftM_sl26_ll13_pl13_dm384_nh6_el2_dl1_df1536_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0/checkpoint.pth'
```

**Verify the path contains**: `dm384_nh6_df1536`

### Step 4: Run Fine-tuning

```bash
cd Time-Series-Library
bash scripts/short_term_forecast/TimesNet_total_finetune.sh
```

You should see:

```
‚ö†Ô∏è  ============================================
‚ö†Ô∏è  ARCHITECTURE COMPATIBILITY CHECK
‚ö†Ô∏è  ============================================
Fine-tuning script requires: d_model=384, n_heads=6, d_ff=1536
Pretrained checkpoint: /content/drive/MyDrive/Model pths/.../checkpoint.pth
Please verify the checkpoint path contains: dm384_nh6_df1536
‚ö†Ô∏è  ============================================

Loading pretrained model from: /content/drive/MyDrive/Model pths/.../checkpoint.pth
‚úÖ Pretrained model loaded successfully!
```

---

## üìä Architecture Parameters

### Current Configuration (All Scripts)

All scripts now use **consistent architecture**:

```python
d_model = 384      # Model embedding dimension
d_ff = 1536        # Feed-forward dimension (4x d_model)
n_heads = 6        # Number of attention heads
e_layers = 2       # Number of encoder layers
d_layers = 1       # Number of decoder layers
```

### Scripts Using This Architecture

‚úÖ `TimesNet_total.sh` - Train from scratch  
‚úÖ `TimesNet_total_finetune.sh` - Fine-tune from M4  
‚úÖ `TimesNet_M4_Weekly_Daily.sh` - M4 pretraining  

---

## üîç Troubleshooting

### Error: "unrecognized arguments: --pretrained_checkpoint"

**Solution**: Update to the latest code. The argument has been added to `run.py`.

### Error: Size mismatch when loading checkpoint

**Cause**: Architecture mismatch between pretrained and fine-tuning models.

**Solution**: 
1. Check checkpoint path for `dmXXX_nhXX_dfXXXX`
2. Verify it matches `dm384_nh6_df1536`
3. If not, either:
   - Find a different checkpoint with matching architecture
   - Or retrain M4 with `TimesNet_M4_Weekly_Daily.sh` (which uses 384/6/1536)

### Warning: "Pretrained checkpoint not found"

**Cause**: Checkpoint path is incorrect or file doesn't exist.

**Solution**: 
1. Check the file exists: `ls -lh /content/drive/MyDrive/Model\ pths/time_series/M4_Weekly/*/checkpoint.pth`
2. Copy the correct full path
3. Update `PRETRAINED_CHECKPOINT` variable in script

---

## üí° Tips

1. **Always match architectures** - This is the #1 cause of fine-tuning failures
2. **Use lower learning rate** - Fine-tuning uses 0.0001 vs training's 0.001
3. **Fewer epochs needed** - Fine-tuning uses 20 epochs vs training's 50
4. **Organize checkpoints** - Use descriptive checkpoint directories
5. **Monitor early stopping** - Patience is 5 for fine-tuning vs 3 for training

---

## üìÅ Checkpoint Organization

```
/content/drive/MyDrive/Model pths/time_series/
‚îú‚îÄ‚îÄ Battery/              # From-scratch training
‚îú‚îÄ‚îÄ Battery_Finetuned/    # Fine-tuned from M4
‚îú‚îÄ‚îÄ M4_Daily/            # M4 Daily pretraining
‚îî‚îÄ‚îÄ M4_Weekly/           # M4 Weekly pretraining (‚Üê use this for fine-tuning)
```

Choose **M4_Weekly** for battery fine-tuning as it has similar temporal patterns!

